{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2024 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZXn1Salxl_w"
      },
      "source": [
        "# Gemini API: All about tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FIB-JDtxgUE"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Counting_Tokens.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRzxdrjKLTJa"
      },
      "source": [
        "An understanding of tokens is central to using the Gemini API. This guide will provide a interactive introduction to what tokens are and how they are used in the Gemini API.\n",
        "\n",
        "## About tokens\n",
        "\n",
        "LLMs break up their input and produce their output at a granularity that is smaller than a word, but larger than a single character or code-point.\n",
        "\n",
        "These **tokens** can be single characters, like `z`, or whole words, like `the`. Long words may be broken up into several tokens. The set of all tokens used by the model is called the vocabulary, and the process of breaking down text into tokens is called tokenization.\n",
        "\n",
        "For Gemini models, a token is equivalent to about 4 characters. **100 tokens are about 60-80 English words**.\n",
        "\n",
        "When billing is enabled, the price of a paid request is controlled by the [number of input and output tokens](https://ai.google.dev/pricing), so knowing how to count your tokens is important.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwJ1lyGC_Ia4"
      },
      "source": [
        "## Tokens in the Gemini API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBa_hMFneZKO"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzsRfmWrxd_F"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IG_wSwTJ2wAP"
      },
      "source": [
        "To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see the [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) quickstart for an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyWgDDHr1yxd"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etlFvMXP3Gb7"
      },
      "source": [
        "### Context windows\n",
        "\n",
        "The models available through the Gemini API have context windows that are measured in tokens. These define how much input you can provide, and how much output the model can generate, and combined are referred to as the \"context window\". This information is available directly through [the API](https://ai.google.dev/api/rest/v1/models/get) and in the [models](https://ai.google.dev/models/gemini) documentation.\n",
        "\n",
        "In this example you can see the `gemini-1.5-flash-latest` model has an 1M tokens context window."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QC23D2z3GLV"
      },
      "outputs": [],
      "source": [
        "model_info = genai.get_model('models/gemini-1.5-flash-latest')\n",
        "(model_info.input_token_limit, model_info.output_token_limit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkh8v5QI4v5h"
      },
      "source": [
        "## Counting tokens\n",
        "\n",
        "The API provides an endpoint for counting the number of tokens in a request: [`GenerativeModel.count_tokens`](https://ai.google.dev/api/python/google/generativeai/GenerativeModel#count_tokens). You pass the same arguments as you would to [`GenerativeModel.generate_content`](https://ai.google.dev/api/python/google/generativeai/GenerativeModel#generate_content) and the service will return the number of tokens in that request."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0J8JPYbCGnv"
      },
      "source": [
        "### Text tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jpoJFpX5Cu_"
      },
      "outputs": [],
      "source": [
        "model = genai.GenerativeModel('models/gemini-1.5-flash-latest')\n",
        "model.count_tokens(\"The quick brown fox jumps over the lazy dog.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0661517a2417"
      },
      "source": [
        "When you call `GenerativeModel.generate_content` (or `ChatSession.send_message`) the response object has a `usage_metadata` attribute containing both the input and output token counts (`prompt_token_count` and `candidates_token_count`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71aa6568a670"
      },
      "outputs": [],
      "source": [
        "response = model.generate_content(\"The quick brown fox jumps over the lazy dog.\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dacccfcbf5f"
      },
      "outputs": [],
      "source": [
        "response.usage_metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQzJ7asV-HJB"
      },
      "source": [
        "### Multi-turn tokens\n",
        "\n",
        "Multi-turn conversational (chat) objects work similarly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqUpyE_E95_w"
      },
      "outputs": [],
      "source": [
        "chat = model.start_chat(history=[{'role':'user', 'parts':'Hi my name is Bob'},  {'role':'model', 'parts':'Hi Bob!'}])\n",
        "model.count_tokens(chat.history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68ae99485a0c"
      },
      "outputs": [],
      "source": [
        "chat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMvjgRkVAvVN"
      },
      "source": [
        "To understand how big your next conversational turn will be, you will need to append it to the history when you call `count_tokens`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxVsykc5A5he"
      },
      "outputs": [],
      "source": [
        "from google.generativeai.types.content_types import to_contents\n",
        "model.count_tokens(chat.history + to_contents('What is the meaning of life?'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZYcaUXl-Sna"
      },
      "source": [
        "### Multi-modal tokens\n",
        "\n",
        "All input to the API is tokenized, including images or other non-text modalities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsKfX8LYAdLv"
      },
      "outputs": [],
      "source": [
        "!curl -L https://goo.gle/instrument-img -o organ.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jzwrahub-ez5"
      },
      "outputs": [],
      "source": [
        "import PIL\n",
        "from IPython.display import display, Image\n",
        "\n",
        "display(Image('organ.jpg', width=300))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4164419d70f"
      },
      "source": [
        "#### Inline content\n",
        "\n",
        "Media objects can be sent to the API inline with the request:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ledzam3H__Ob"
      },
      "outputs": [],
      "source": [
        "organ = PIL.Image.open('organ.jpg')\n",
        "model.count_tokens(['Tell me about this instrument', organ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3851a09ec17"
      },
      "source": [
        "#### Files API\n",
        "\n",
        "The model sees identical tokens if you upload parts of the prompt through the files API instead:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f994c2dd6e05"
      },
      "outputs": [],
      "source": [
        "organ_upload = genai.upload_file('organ.jpg')\n",
        "\n",
        "model.count_tokens(['Tell me about this instrument', organ_upload])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXF0vpdG_H_Q"
      },
      "source": [
        "### Media token counts\n",
        "\n",
        "Internally, images are a fixed size, so they consume a fixed number of tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPPfXRJiA3KV"
      },
      "outputs": [],
      "source": [
        "!curl -O \"https://storage.googleapis.com/generativeai-downloads/images/jetpack.jpg\" --silent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqG83Rko8UpG"
      },
      "outputs": [],
      "source": [
        "jetpack = PIL.Image.open('jetpack.jpg')\n",
        "display(Image('jetpack.jpg', width=300))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hc0CBsl6_Tkk"
      },
      "outputs": [],
      "source": [
        "print(organ.size)\n",
        "print(model.count_tokens(organ))\n",
        "\n",
        "print(jetpack.size)\n",
        "print(model.count_tokens(jetpack))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8342199c9eb4"
      },
      "source": [
        "Audio and video are each converted to tokens at a fixed rate of tokens per minute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "be103816898c"
      },
      "outputs": [],
      "source": [
        "!curl -q -o sample.mp3  \"https://storage.googleapis.com/generativeai-downloads/data/State_of_the_Union_Address_30_January_1961.mp3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ada734553530"
      },
      "outputs": [],
      "source": [
        "audio_sample = genai.upload_file('sample.mp3')\n",
        "model.count_tokens(audio_sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9367d1afac3"
      },
      "source": [
        "### System instructions and tools\n",
        "\n",
        "System instructions and tools also count towards the total:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2a83ac75dfe"
      },
      "outputs": [],
      "source": [
        "genai.GenerativeModel().count_tokens(\"The quick brown fox jumps over the lazy dog.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c275fafdf080"
      },
      "outputs": [],
      "source": [
        "genai.GenerativeModel(system_instruction='Talk like a pirate!').count_tokens(\"The quick brown fox jumps over the lazy dog.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fcff3d1403e"
      },
      "outputs": [],
      "source": [
        "def add(a:float, b:float):\n",
        "    \"\"\"returns a + b.\"\"\"\n",
        "    return a+b\n",
        "\n",
        "def subtract(a:float, b:float):\n",
        "    \"\"\"returns a - b.\"\"\"\n",
        "    return a-b\n",
        "\n",
        "def multiply(a:float, b:float):\n",
        "    \"\"\"returns a * b.\"\"\"\n",
        "    return a*b\n",
        "\n",
        "def divide(a:float, b:float):\n",
        "    \"\"\"returns a / b.\"\"\"\n",
        "    return a*b\n",
        "\n",
        "model = genai.GenerativeModel(model_name='gemini-1.5-flash-latest',\n",
        "                              tools=[add, subtract, multiply, divide])\n",
        "model.count_tokens(\"The quick brown fox jumps over the lazy dog.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfZNBWZLDCXa"
      },
      "source": [
        "## Further reading\n",
        "\n",
        "For more on token counting, check out the API reference.\n",
        "\n",
        "* [countTokens](https://ai.google.dev/api/rest/v1/models/countTokens) REST API reference,\n",
        "* [count_tokens](https://ai.google.dev/api/python/google/generativeai/GenerativeModel#count_tokens) Python API reference,"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Counting_Tokens.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
