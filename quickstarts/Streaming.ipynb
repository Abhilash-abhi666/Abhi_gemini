{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2024 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeadDkMiISin"
      },
      "source": [
        "# Gemini API: Streaming Quickstart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df1767a3d1cc"
      },
      "source": [
        "This notebook demonstrates streaming in the Python SDK. By default, the Python SDK returns a response after the model completes the entire generation process. You can also stream the response as it is being generated, and the model will return chunks of the response as soon as they are generated.\n",
        "\n",
        "**Download this notebook and run it locally (not in Google Colab)**\n",
        "\n",
        "Streaming is not handled correctly in Google Colab yet. Currently all the stream chunks are returned together, not as they are generated. To see the correct behavior, download this notebook and run it locally using Jupyter, instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuiLSV7amy3P"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q google-generativeai # Install the Python SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79EWm0DAmy-g"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkeZNMrw6kPD"
      },
      "source": [
        "You'll need an API key stored in an environment variable to run this notebook. See the the [Authentication quickstart](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) for an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9O-OzeAKC_m"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "genai.configure(api_key=os.environ['GOOGLE_API_KEY'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUoa5q0iUuE1"
      },
      "source": [
        "## Handle streaming responses\n",
        "\n",
        "To stream responses, use [`GenerativeModel.generate_content(..., stream=True)`](https://ai.google.dev/api/python/google/generativeai/GenerativeModel#generate_content).\n",
        "\n",
        "**Note**: This cell runs with a Google Colab runtime, but does not properly show streaming due to implementation details of Colab runtimes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVWWGBsBok3m"
      },
      "outputs": [],
      "source": [
        "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
        "response = model.generate_content(\"Write a cute story about cats.\", stream=True)\n",
        "for chunk in response:\n",
        "    print(chunk.text)\n",
        "    print(\"_\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KswwVyHCKC_n"
      },
      "source": [
        "## Handle streaming responses asynchronously\n",
        "\n",
        "To stream responses asynchronously, use [`GenerativeModel.generate_content_async(..., stream=True)`](https://ai.google.dev/api/python/google/generativeai/GenerativeModel#generate_content_async).\n",
        "\n",
        "**Note**: These cells do NOT work with a Google Colab runtime, but do work in a local Jupyter notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6sXnWrJoKoo"
      },
      "outputs": [],
      "source": [
        "async for chunk in await model.generate_content_async(\"Write a cute story about cats.\", stream=True):\n",
        "    if chunk.text:\n",
        "        print(chunk.text)\n",
        "    print(\"_\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpK3p1B4KC_o"
      },
      "source": [
        "Here's a simple example of two asynchronous functions running simultaneously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJ-8SjYwKC_o"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "\n",
        "async def get_response():\n",
        "    async for chunk in await model.generate_content_async(\"Write a cute story about cats.\", stream=True):\n",
        "        if chunk.text:\n",
        "            print(chunk.text)\n",
        "        print(\"_\"*80)\n",
        "\n",
        "async def something_else():\n",
        "    for i in range(5):\n",
        "        print(\"==========not blocked!==========\")\n",
        "        await asyncio.sleep(3)\n",
        "\n",
        "async def async_demo():\n",
        "    # Create tasks\n",
        "    task1 = asyncio.create_task(get_response())\n",
        "    task2 = asyncio.create_task(something_else())\n",
        "\n",
        "    # Wait for tasks to complete\n",
        "    await asyncio.gather(task1, task2)\n",
        "\n",
        "# Jupyter notebooks handle event loops for you, so await directly\n",
        "await async_demo()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Streaming.ipynb",
      "toc_visible": true
    },
    "google": {
      "image_path": "/site-assets/images/share.png",
      "keywords": [
        "examples",
        "googleai",
        "samplecode",
        "python",
        "embed",
        "function"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
