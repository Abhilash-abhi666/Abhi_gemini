{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeadDkMiISin"
      },
      "source": [
        "# Gemini API: Function Calling Quickstart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEXQ3OwKIa-O"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/gemini-api-cookbook/blob/main/quickstarts/Function_calling.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df1767a3d1cc"
      },
      "source": [
        "You can provide Gemini models with descriptions of functions. The model may ask you to call a function and send it back the result to help the model handle your query."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFNV1e3ASJha"
      },
      "source": [
        "### Install the Python SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9OEoeosRTv-5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/137.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/137.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m133.1/137.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.4/137.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -U -q google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TS9l5igubpHO"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHYFrFPjSGNq"
      },
      "source": [
        "## Configure your API key\n",
        "\n",
        "To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see  [Authentication](https://github.com/google-gemini/gemini-api-cookbook/blob/main/quickstarts/Authentication.ipynb) for an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ab9ASynfcIZn"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f383614ec30"
      },
      "source": [
        "## Function Basics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b82c1aecb657"
      },
      "source": [
        "You can pass a list of functions to the `tools` argument when creating a `genai.GenerativeModel`.\n",
        "\n",
        "> Important: The SDK converts the function's argument's type annotations to a format the API understands (`glm.FunctionDeclaration`). The API only supports a limited selection of argument types, and this automatic conversion only supports a subset of that: `AllowedTypes = int | float | bool | str | list['AllowedTypes'] | dict`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "42b27b02d2f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "genai.GenerativeModel(\n",
              "    model_name='models/gemini-1.0-pro',\n",
              "    generation_config={},\n",
              "    safety_settings={},\n",
              "    tools=<google.generativeai.types.content_types.FunctionLibrary object at 0x797640d2dab0>,\n",
              ")"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def add(a:float, b:float):\n",
        "    \"\"\"returns a + b.\"\"\"\n",
        "    return a+b\n",
        "\n",
        "def subtract(a:float, b:float):\n",
        "    \"\"\"returns a - b.\"\"\"\n",
        "    return a*b\n",
        "\n",
        "def multiply(a:float, b:float):\n",
        "    \"\"\"returns a * b.\"\"\"\n",
        "    return a*b\n",
        "\n",
        "def divide(a:float, b:float):\n",
        "    \"\"\"returns a / b.\"\"\"\n",
        "    return a*b\n",
        "\n",
        "model = genai.GenerativeModel(model_name='gemini-1.0-pro',\n",
        "                              tools=[add, subtract, multiply, divide])\n",
        "\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5fd91032a1e"
      },
      "source": [
        "The recomended way to use function calling is through the chat interface. The main reason is that `FunctionCalls` fit nicely into chat's multi-turn structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "d3b91c855257"
      },
      "outputs": [],
      "source": [
        "chat = model.start_chat(enable_automatic_function_calling=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1481a6159399"
      },
      "source": [
        "With automatic function calling enabled `chat.send_message` automatically calls your function if the model asks it to.\n",
        "\n",
        "It appears to simply return a text response, containing the correct answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "81d8def3d865"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'It is 2508 mittens in total.'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = chat.send_message('I have 57 cats, each owns 44 mittens, how many mittens is that in total?')\n",
        "response.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "951c0f83f72e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2508"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "57*44"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7731e35f2383"
      },
      "source": [
        "If you look in the `ChatSession.history` you can the sequence of `glm.Content` objects representing the turns of the conversation.\n",
        "Each `glm.Content` contains a list of `glm.Part` objects that either contain text, a `glm.FunctionCall` or a `glm.FunctionResponse`.\n",
        "\n",
        "Here the sequence of turns is:\n",
        "\n",
        "1. You sent the question.\n",
        "2. The model replied with a `glm.FunctionCall`.\n",
        "3. The `genai.ChatSession` executed the function locally and sent the model back a `glm.FunctionResponse`.\n",
        "4. The model used the function output in its answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9f7eff1e8e60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "user -> [{'text': 'I have 57 cats, each owns 44 mittens, how many mittens is that in total?'}]\n",
            "--------------------------------------------------------------------------------\n",
            "model -> [{'function_call': {'name': 'multiply', 'args': {'b': 44.0, 'a': 57.0}}}]\n",
            "--------------------------------------------------------------------------------\n",
            "user -> [{'function_response': {'name': 'multiply', 'response': {'result': 2508.0}}}]\n",
            "--------------------------------------------------------------------------------\n",
            "model -> [{'text': 'It is 2508 mittens in total.'}]\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for content in chat.history:\n",
        "    print(content.role, \"->\", [type(part).to_dict(part) for part in content.parts])\n",
        "    print('-'*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2471fd72f05e"
      },
      "source": [
        "In general the state diagram is:\n",
        "\n",
        "<img src=\"https://ai.google.dev/images/tutorials/function_call_state_diagram.png\" alt=\"The model can always reply with text, or a FunctionCall. Iff the model sends a FunctionCall the user must reply with a FunctionResponse\" width=50%>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f42d69800cff"
      },
      "source": [
        "The model can respond with multiple function calls before returning a text response, and function calls come before the text response."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9610f3465a69"
      },
      "source": [
        "While this was all handled automatically, if you need more control, you can:\n",
        "\n",
        "- Leave the default `enable_automatic_function_calling=False` and process the `glm.FunctionCall` responses yourself.\n",
        "- Or use `GenerativeModel.generate_content`, where you also need to manage the chat history."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eea8e3a0b89f"
      },
      "source": [
        "## Another example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34ffab0bf365"
      },
      "source": [
        "On the FunctionCalling overview page, there's a [movie finding example](https://ai.google.dev/docs/function_calling#function-calling-single-turn-curl-sample), here's the rough equivalent, in python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "46ba0fa3d09a"
      },
      "outputs": [],
      "source": [
        "def find_movies(description: str, location: str = ''):\n",
        "  \"\"\"find movie titles currently playing in theaters based on any description, genre, title words, etc.\n",
        "\n",
        "  Args:\n",
        "      description: Any kind of description including category or genre, title words, attributes, etc.\n",
        "      location: The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\n",
        "  \"\"\"\n",
        "  pass\n",
        "\n",
        "def find_theaters(location: str, movie: str = ''):\n",
        "    \"\"\"Find theaters based on location and optionally movie title which are is currently playing in theaters.\n",
        "\n",
        "    Args:\n",
        "        location: The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\n",
        "        movie: Any movie title\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "def get_showtimes(location:str, movie:str, theater:str, date:str):\n",
        "    \"\"\"\n",
        "    Find the start times for movies playing in a specific theater.\n",
        "\n",
        "    Args:\n",
        "      location: The city and state, e.g. San Francisco, CA or a zip code e.g. 95616\n",
        "      movie: Any movie title\n",
        "      thearer: Name of the theater\n",
        "      date: Date for requested showtime\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "model = genai.GenerativeModel(model_name='gemini-1.0-pro',\n",
        "                              tools=[find_movies, find_theaters, get_showtimes])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11631c6e2b10"
      },
      "source": [
        "Ask the model in natural languag and it returns a function call:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5e3b9c84d883"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[function_call {\n",
              "  name: \"find_theaters\"\n",
              "  args {\n",
              "    fields {\n",
              "      key: \"location\"\n",
              "      value {\n",
              "        string_value: \"Mountain View, CA\"\n",
              "      }\n",
              "    }\n",
              "    fields {\n",
              "      key: \"movie\"\n",
              "      value {\n",
              "        string_value: \"Barbie\"\n",
              "      }\n",
              "    }\n",
              "  }\n",
              "}\n",
              "]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = model.generate_content(\"Which theaters in Mountain View show the Barbie movie?\")\n",
        "response.candidates[0].content.parts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0a3173919ca"
      },
      "source": [
        "## Next Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c2f31504490"
      },
      "source": [
        "Useful API references:\n",
        "\n",
        "- The [genai.GenerativeModel](https://ai.google.dev/api/python/google/generativeai/GenerativeModel) class\n",
        "  - Its [GenerativeModel.generate_content](https://ai.google.dev/api/python/google/generativeai/GenerativeModel#generate_content) method builds a [glm.GenerateContentRequest](https://ai.google.dev/api/python/google/ai/generativelanguage/GenerateContentRequest) behind the scenes.\n",
        "    - The request's `.tools` field contains a list of 1 [glm.Tool](https://ai.google.dev/api/python/google/ai/generativelanguage/Tool) object.\n",
        "    - The tool's `function_declarations` attribute contains a list of [FunctionDeclarations](https://ai.google.dev/api/python/google/ai/generativelanguage/FunctionDeclaration) objects.\n",
        "- The [response](https://ai.google.dev/api/python/google/ai/generativelanguage/GenerateContentResponse) may contain a [glm.FunctionCall](https://ai.google.dev/api/python/google/ai/generativelanguage/FunctionCall), in `response.candidates[0].contents.parts[0]`.\n",
        "- if `enable_automatic_function_calling` is set the [genai.ChatSession](https://ai.google.dev/api/python/google/generativeai/ChatSession) executes the call, and sends back the [glm.FunctionResponse](https://ai.google.dev/api/python/google/ai/generativelanguage/FunctionResponse).\n",
        "- In response to a [FunctionCall](https://ai.google.dev/api/python/google/ai/generativelanguage/FunctionCall) the model always expects a [FunctionResponse](https://ai.google.dev/api/python/google/ai/generativelanguage/FunctionResponse).\n",
        "- If you reply manually using [chat.send_message](https://ai.google.dev/api/python/google/generativeai/ChatSession#send_message) or [model.generate_content](https://ai.google.dev/api/python/google/generativeai/GenerativeModel#generate_content) remember thart the API is stateless you have to send the whole conversation history (a list of [content](https://ai.google.dev/api/python/google/ai/generativelanguage/Content) objects), not just the last one containing the `FunctionResponse`."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Function_calling.ipynb",
      "toc_visible": true
    },
    "google": {
      "image_path": "/site-assets/images/share.png",
      "keywords": [
        "examples",
        "googleai",
        "samplecode",
        "python",
        "embed",
        "function"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
