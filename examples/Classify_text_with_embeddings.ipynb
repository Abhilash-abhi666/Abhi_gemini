{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2024 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STuxHh6kk3eL"
      },
      "source": [
        "# Classify text with embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUmTFPw2W_UD"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Classify_text_with_embeddings.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhT1u-Pof10V"
      },
      "source": [
        "## Overview\n",
        "\n",
        "In this notebook, you'll learn to use the embeddings produced by the Gemini API to train a model that can classify different types of newsgroup posts based on the topic.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXq0ygI3BCdQ"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiJjB2vWCQJP"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import tqdm\n",
        "import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import google.generativeai as genai\n",
        "import google.ai.generativelanguage as glm\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras import layers\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import sklearn.metrics as skmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mwJYXpElYJc"
      },
      "source": [
        "To run the following cell, your API key must be stored it in a Colab Secret named `GOOGLE_API_KEY`. If you don't already have an API key, or you're not sure how to create a Colab Secret, see the [Authentication](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Authentication.ipynb) quickstart for an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tayrk_A2lZ7A"
      },
      "outputs": [],
      "source": [
        "API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5B9sWq0hNEV"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "The [20 Newsgroups Text Dataset](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html){:.external} contains 18,000 newsgroups posts on 20 topics divided into training and test sets. The split between the training and test datasets are based on messages posted before and after a specific date. For this tutorial, you will be using the subsets of the training and test datasets. You will preprocess and organize the data into Pandas dataframes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDoKis4om-Ea"
      },
      "outputs": [],
      "source": [
        "newsgroups_train = fetch_20newsgroups(subset='train')\n",
        "newsgroups_test = fetch_20newsgroups(subset='test')\n",
        "\n",
        "# View list of class names for dataset\n",
        "newsgroups_train.target_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDz9MjkNl_FD"
      },
      "source": [
        "Here is an example of what a data point from the training set looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPq-56AimOPX"
      },
      "outputs": [],
      "source": [
        "idx = newsgroups_train.data[0].index('Lines')\n",
        "print(newsgroups_train.data[0][idx:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9-DD7wgCx8j"
      },
      "source": [
        "Now you will begin preprocessing the data for this tutorial. Remove any sensitive information like names, email, or redundant parts of the text like `\"From: \"` and `\"\\nSubject: \"`. Organize the information into a Pandas dataframe so it is more readable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urpLwp3UmPF3"
      },
      "outputs": [],
      "source": [
        "def preprocess_newsgroup_data(newsgroup_dataset):\n",
        "  # Apply functions to remove names, emails, and extraneous words from data points in newsgroups.data\n",
        "  newsgroup_dataset.data = [re.sub(r'[\\w\\.-]+@[\\w\\.-]+', '', d) for d in newsgroup_dataset.data] # Remove email\n",
        "  newsgroup_dataset.data = [re.sub(r\"\\([^()]*\\)\", \"\", d) for d in newsgroup_dataset.data] # Remove names\n",
        "  newsgroup_dataset.data = [d.replace(\"From: \", \"\") for d in newsgroup_dataset.data] # Remove \"From: \"\n",
        "  newsgroup_dataset.data = [d.replace(\"\\nSubject: \", \"\") for d in newsgroup_dataset.data] # Remove \"\\nSubject: \"\n",
        "\n",
        "  # Cut off each text entry after 5,000 characters\n",
        "  newsgroup_dataset.data = [d[0:5000] if len(d) > 5000 else d for d in newsgroup_dataset.data]\n",
        "\n",
        "  # Put data points into dataframe\n",
        "  df_processed = pd.DataFrame(newsgroup_dataset.data, columns=['Text'])\n",
        "  df_processed['Label'] = newsgroup_dataset.target\n",
        "  # Match label to target name index\n",
        "  df_processed['Class Name'] = ''\n",
        "  for idx, row in df_processed.iterrows():\n",
        "    df_processed.at[idx, 'Class Name'] = newsgroup_dataset.target_names[row['Label']]\n",
        "\n",
        "  return df_processed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMKddQdNnAOV"
      },
      "outputs": [],
      "source": [
        "# Apply preprocessing function to training and test datasets\n",
        "df_train = preprocess_newsgroup_data(newsgroups_train)\n",
        "df_test = preprocess_newsgroup_data(newsgroups_test)\n",
        "\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogEGbg5XDv-T"
      },
      "source": [
        "Next, you will sample some of the data by taking 100 data points in the training dataset, and dropping a few of the categories to run through this tutorial. Choose the science categories to compare."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2N7xXhJohLR"
      },
      "outputs": [],
      "source": [
        "def sample_data(df, num_samples, classes_to_keep):\n",
        "  df = df.groupby('Label', as_index = False).apply(lambda x: x.sample(num_samples)).reset_index(drop=True)\n",
        "\n",
        "  df = df[df['Class Name'].str.contains(classes_to_keep)]\n",
        "\n",
        "  # Reset the encoding of the labels after sampling and dropping certain categories\n",
        "  df['Class Name'] = df['Class Name'].astype('category')\n",
        "  df['Encoded Label'] = df['Class Name'].cat.codes\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jS2g_ZGupBUb"
      },
      "outputs": [],
      "source": [
        "TRAIN_NUM_SAMPLES = 100\n",
        "TEST_NUM_SAMPLES = 25\n",
        "CLASSES_TO_KEEP = 'sci' # Class name should contain 'sci' in it to keep science categories\n",
        "df_train = sample_data(df_train, TRAIN_NUM_SAMPLES, CLASSES_TO_KEEP)\n",
        "df_test = sample_data(df_test, TEST_NUM_SAMPLES, CLASSES_TO_KEEP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j04TMPY8rV5q"
      },
      "outputs": [],
      "source": [
        "df_train.value_counts('Class Name')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMsnfkVDsJlU"
      },
      "outputs": [],
      "source": [
        "df_test.value_counts('Class Name')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kr-WlKzXjYWn"
      },
      "source": [
        "## Create the embeddings\n",
        "\n",
        "In this section, you will see how to generate embeddings for a piece of text using the embeddings from the Gemini API. To learn more about embeddings, visit the [embeddings guide](https://ai.google.dev/docs/embeddings_guide).\n",
        "\n",
        "**NOTE**: Embeddings are computed one at a time, large sample sizes can take a long time!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPECMeE2xYA_"
      },
      "source": [
        "### API changes to Embeddings embedding-001\n",
        "\n",
        "For the new embeddings model, there is a new task type parameter and the optional title (only valid with task_type=`RETRIEVAL_DOCUMENT`).\n",
        "\n",
        "These new parameters apply only to the newest embeddings models.The task types are:\n",
        "\n",
        "Task Type | Description\n",
        "---       | ---\n",
        "RETRIEVAL_QUERY\t| Specifies the given text is a query in a search/retrieval setting.\n",
        "RETRIEVAL_DOCUMENT | Specifies the given text is a document in a search/retrieval setting.\n",
        "SEMANTIC_SIMILARITY\t| Specifies the given text will be used for Semantic Textual Similarity (STS).\n",
        "CLASSIFICATION\t| Specifies that the embeddings will be used for classification.\n",
        "CLUSTERING\t| Specifies that the embeddings will be used for clustering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTBGKkPQsotz"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "from google.api_core import retry\n",
        "\n",
        "def make_embed_text_fn(model):\n",
        "\n",
        "  @retry.Retry(timeout=300.0)\n",
        "  def embed_fn(text: str) -> list[float]:\n",
        "    # Set the task_type to CLASSIFICATION.\n",
        "    embedding = genai.embed_content(model=model,\n",
        "                                    content=text,\n",
        "                                    task_type=\"classification\")\n",
        "    return embedding['embedding']\n",
        "\n",
        "  return embed_fn\n",
        "\n",
        "def create_embeddings(model, df):\n",
        "  df['Embeddings'] = df['Text'].progress_apply(make_embed_text_fn(model))\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AH0yrHUHtHtw"
      },
      "outputs": [],
      "source": [
        "model = 'models/embedding-001'\n",
        "df_train = create_embeddings(model, df_train)\n",
        "df_test = create_embeddings(model, df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6G5TvLlmRjHc"
      },
      "outputs": [],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPYEYkIsWt_5"
      },
      "source": [
        "## Build a simple classification model\n",
        "Here you will define a simple model with one hidden layer and a single class probability output. The prediction will correspond to the probability of a piece of text being a particular class of news. When you build your model, Keras will automatically shuffle the data points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oLGi4w5JsQR"
      },
      "outputs": [],
      "source": [
        "def build_classification_model(input_size: int, num_classes: int) -> keras.Model:\n",
        "  inputs = x = keras.Input(input_size)\n",
        "  x = layers.Dense(input_size, activation='relu')(x)\n",
        "  x = layers.Dense(num_classes, activation='sigmoid')(x)\n",
        "  return keras.Model(inputs=[inputs], outputs=x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kORA1Akl5GsG"
      },
      "outputs": [],
      "source": [
        "# Derive the embedding size from the first training element.\n",
        "embedding_size = len(df_train['Embeddings'].iloc[0])\n",
        "\n",
        "# Give your model a different name, as you have already used the variable name 'model'\n",
        "classifier = build_classification_model(embedding_size, len(df_train['Class Name'].unique()))\n",
        "classifier.summary()\n",
        "\n",
        "classifier.compile(loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                   optimizer = keras.optimizers.Adam(learning_rate=0.001),\n",
        "                   metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPYYKnqFvt9x"
      },
      "outputs": [],
      "source": [
        "embedding_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbpTGGiMXDxl"
      },
      "source": [
        "## Train the model to classify newsgroups\n",
        "\n",
        "Finally, you can train a simple model. Use a small number of epochs to avoid overfitting. The first epoch takes much longer than the rest, because the embeddings need to be computed only once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGgvMZGfJ1A4"
      },
      "outputs": [],
      "source": [
        "NUM_EPOCHS = 20\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Split the x and y components of the train and validation subsets.\n",
        "y_train = df_train['Encoded Label']\n",
        "x_train = np.stack(df_train['Embeddings'])\n",
        "y_val = df_test['Encoded Label']\n",
        "x_val = np.stack(df_test['Embeddings'])\n",
        "\n",
        "# Train the model for the desired number of epochs.\n",
        "callback = keras.callbacks.EarlyStopping(monitor='accuracy', patience=3)\n",
        "\n",
        "history = classifier.fit(x=x_train,\n",
        "                         y=y_train,\n",
        "                         validation_data=(x_val, y_val),\n",
        "                         callbacks=[callback],\n",
        "                         batch_size=BATCH_SIZE,\n",
        "                         epochs=NUM_EPOCHS,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGBaDHZUPdJO"
      },
      "source": [
        "## Evaluate model performance\n",
        "\n",
        "Use Keras <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/Model#evaluate\"><code>Model.evaluate</code></a> to get the loss and accuracy on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2kOeiqqQIB8"
      },
      "outputs": [],
      "source": [
        "classifier.evaluate(x=x_val, y=y_val, return_dict=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyxMhiLYQXAN"
      },
      "source": [
        "One way to evaluate your model performance is to visualize the classifier performance. Use `plot_history` to see the loss and accuracy trends over the epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MaDO9hwbEOW3"
      },
      "outputs": [],
      "source": [
        "def plot_history(history):\n",
        "  \"\"\"\n",
        "    Plotting training and validation learning curves.\n",
        "\n",
        "    Args:\n",
        "      history: model history with all the metric measures\n",
        "  \"\"\"\n",
        "  fig, (ax1, ax2) = plt.subplots(1,2)\n",
        "  fig.set_size_inches(20, 8)\n",
        "\n",
        "  # Plot loss\n",
        "  ax1.set_title('Loss')\n",
        "  ax1.plot(history.history['loss'], label = 'train')\n",
        "  ax1.plot(history.history['val_loss'], label = 'test')\n",
        "  ax1.set_ylabel('Loss')\n",
        "\n",
        "  ax1.set_xlabel('Epoch')\n",
        "  ax1.legend(['Train', 'Validation'])\n",
        "\n",
        "  # Plot accuracy\n",
        "  ax2.set_title('Accuracy')\n",
        "  ax2.plot(history.history['accuracy'],  label = 'train')\n",
        "  ax2.plot(history.history['val_accuracy'], label = 'test')\n",
        "  ax2.set_ylabel('Accuracy')\n",
        "  ax2.set_xlabel('Epoch')\n",
        "  ax2.legend(['Train', 'Validation'])\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "plot_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOgva0pbP4FS"
      },
      "source": [
        "Another way to view model performance, beyond just measuring loss and accuracy is to use a confusion matrix. The confusion matrix allows you to assess the performance of the classification model beyond accuracy. You can see what misclassified points get classified as. In order to build the confusion matrix for this multi-class classification problem, get the actual values in the test set and the predicted values.\n",
        "\n",
        "Start by generating the predicted class for each example in the validation set using [`Model.predict()`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#predict)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRUx5ao9QRcO"
      },
      "outputs": [],
      "source": [
        "y_hat = classifier.predict(x=x_val)\n",
        "y_hat = np.argmax(y_hat, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVidbr0OT5tL"
      },
      "outputs": [],
      "source": [
        "labels_dict = dict(zip(df_test['Class Name'], df_test['Encoded Label']))\n",
        "labels_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ae76701e178"
      },
      "outputs": [],
      "source": [
        "cm = skmetrics.confusion_matrix(y_val, y_hat)\n",
        "disp = skmetrics.ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=labels_dict.keys())\n",
        "disp.plot(xticks_rotation='vertical')\n",
        "plt.title('Confusion matrix for newsgroup test dataset');\n",
        "plt.grid(False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Classify_text_with_embeddings.ipynb",
      "toc_visible": true
    },
    "google": {
      "image_path": "/examples/train_text_classifier_embeddings_files/output_3ae76701e178_0.png",
      "keywords": [
        "examples",
        "googleai",
        "samplecode",
        "python",
        "embed"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
